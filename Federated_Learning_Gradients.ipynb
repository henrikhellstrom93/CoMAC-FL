{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation settings\n",
    "num_rounds = 50 # Number of communication rounds\n",
    "num_devices = 2 # Number of devices\n",
    "bs = 64 # Batch size for local training at devices\n",
    "ep = 2 # Number of local epochs before communication round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "#Load MNIST dataset\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "#Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\")/255, y_train)\n",
    ")\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_test.reshape(10000, 784).astype(\"float32\")/255, y_test)\n",
    ")\n",
    "#Create batches\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(bs, drop_remainder=True)\n",
    "test_dataset = test_dataset.shuffle(buffer_size=1024).batch(bs, drop_remainder=True)\n",
    "\n",
    "half_dataset1 = dataset.shard(2, 0)\n",
    "half_dataset2 = dataset.shard(2, 1)\n",
    "\n",
    "#Split dataset into shards\n",
    "train_shards = []\n",
    "for i in range(num_devices):\n",
    "    train_shards.append(dataset.shard(num_devices, i))\n",
    "\n",
    "print(\"Dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up DNN models\n",
    "model_template = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model_list = []\n",
    "\n",
    "for i in range(num_devices):\n",
    "    model_list.append(tf.keras.models.clone_model(model_template))\n",
    "\n",
    "global_model = tf.keras.models.clone_model(model_template)\n",
    "    \n",
    "#Define loss function and optimizer, required for \"train_on_batch\" f\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "#Define training function (x = batch of 'bs' training samples, y = corresponding labels, m = model)\n",
    "#There is a wrapper here because of the following error: \n",
    "#'tf.function-decorated function tried to create variables on non-first call.'\n",
    "#This error exists because I'm passing a model to the @tf.function annotated train_on_batch function.\n",
    "#When this function runs for the first time, it will take the model passed to it and use that to \n",
    "#instantiate a computational graph. However, when new models are passed to the same function, it \n",
    "#crashes the program instead. By using this wrapper, it forces the tensorflow backend to create a new \n",
    "#computational graph for the new model.\n",
    "\n",
    "def train_on_batch(x, y, m):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #Forward pass\n",
    "        logits = m(x)\n",
    "        #Loss value for this batch\n",
    "        loss_val = loss_fn(y, logits)\n",
    "        gradients = tape.gradient(loss_val, m.trainable_weights)\n",
    "    opt.apply_gradients(zip(gradients, m.trainable_weights))\n",
    "    return loss_val, gradients\n",
    "\n",
    "for model in model_list:\n",
    "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "    \n",
    "global_model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communication round 1/50\n",
      "Device:  0 Loss:  0.05585908889770508\n",
      "Device:  1 Loss:  0.18742266297340393\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ef106c01812b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m#Update global model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mglobal_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobal_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglobal_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mglobal_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobal_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcum_gradient\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mget_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mget_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[0mWeights\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \"\"\"\n\u001b[1;32m-> 1317\u001b[1;33m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[0moutput_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mweights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    483\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \"\"\"\n\u001b[1;32m--> 485\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dedup_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_undeduplicated_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_undeduplicated_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    488\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_undeduplicated_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[1;34m\"\"\"Returns the undeduplicated list of all layer variables/weights.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\keras\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_assert_weights_created\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1472\u001b[0m                        \u001b[1;34m'Weights are created when the Model is first called on '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m                        \u001b[1;34m'inputs or `build()` is called with an `input_shape`.'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                        self.name)\n\u001b[0m\u001b[0;32m   1475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1476\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_graph_network_add_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msymbolic_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Weights for model sequential have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`."
     ]
    }
   ],
   "source": [
    "#Train DNN\n",
    "acc_history = []\n",
    "for r in range(num_rounds):\n",
    "    print(\"Communication round \" + str(r+1) + \"/\" + str(num_rounds))\n",
    "    start = time.time()\n",
    "    #Local training\n",
    "    gradients = []\n",
    "    for d in range(num_devices): #TODO: Parallelize\n",
    "        acc_list = []\n",
    "        loss_val = 0\n",
    "        cum_gradient = 0\n",
    "        for epoch in range(ep):\n",
    "            for step, (x, y) in enumerate(train_shards[d]):\n",
    "                loss_val, gradient = train_on_batch(x, y, model_list[d])\n",
    "                #Calculate cumulative gradient\n",
    "                for l in range(len(gradient)):\n",
    "                    if cum_gradient == 0:\n",
    "                        cum_gradient = gradient\n",
    "                        continue\n",
    "                    cum_gradient[l] = cum_gradient[l]+gradient[l]\n",
    "            #Divide cumulative gradient by number of batches\n",
    "            for l in range(len(gradient)):\n",
    "                cum_gradient[l] = cum_gradient[l]/len(list(train_shards[d]))\n",
    "        for l in range(len(gradient)):\n",
    "            cum_gradient[l] = cum_gradient[l]/ep\n",
    "        print(\"Device: \", d, \"Loss: \", float(loss_val))\n",
    "            \n",
    "    #Update global model\n",
    "    global_weights = global_model.get_weights()\n",
    "    for l in range(len(global_weights)):\n",
    "        global_weights[l] = global_weights[l] - cum_gradient[l]\n",
    "    \n",
    "    #Set model of all devices to the global_model\n",
    "    for model in model_list:\n",
    "        model.set_weights(global_model.get_weights())\n",
    "    acc_history.append(model_list[0].evaluate(test_dataset, verbose=0)[1])\n",
    "    print(\"Accuracy on test dataset: \", acc_history[-1])\n",
    "    print(str(int(time.time()-start)) + \" seconds elapsed\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(acc_history[1:])), acc_history[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
